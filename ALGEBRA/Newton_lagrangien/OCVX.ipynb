{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method  Newton Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Intro :\n",
    "\n",
    "We propose in this part a presentation of newton method for non linear problem solving (second order here) the first part study how we solve a problem without constraint, the second part show **2 methods** for problem solving \n",
    "with constraints, those 2 methods are 2 differents ways to look at the same thing\n",
    "\n",
    "### Part 1 : Newton method (with no constraints)\n",
    "\n",
    "In one dimensional problem, let consider the function $f(x)$ from $\\R \\longrightarrow \\R$ and $f_T(x)$ the taylor approximation in the point $x_n$ of $f(x_n)$ is : \n",
    "$f_T(x_n+\\Delta x) \\approx f(x_n)+f'(x_n)\\Delta x+\\frac 1 2 f''(x_n) \\Delta x^2$\n",
    "\n",
    "If $x_{n + 1} = x_n+\\Delta x $ is a stationary point, it means that the derivative $\\frac{f_T(x_n+\\Delta x) - f_T(x_n)}{\\Delta x} = \\frac{f'(x_n)\\Delta x+\\frac 1 2 f''(x_n) \\Delta x^2}{\\Delta x} \\implies f'(x_n)\\Delta x+\\frac 1 2 f''(x_n) \\Delta x^2 = 0$\n",
    "so we could find the $\\Delta x$ (the step) either by solving plynomial second order or solving the derivative $f'(x_n)+f'' (x_n) \\Delta x$ which give $\\Delta x = - \\frac{f'(x)}{f''(x)}$\n",
    "\n",
    "#### Higher dimensions : \n",
    "\n",
    "In higher dimension $f'(x) = \\nabla f(x)$ the gradient of $f(x)$ and $f''(x) = \\mathbf{H}f(x) $ the hessian matrix of $f(x)$ so :\n",
    "\n",
    "$$\\Delta x = \\mathbf{H}f(x)^{-1} . \\nabla f(x)$$\n",
    "\n",
    "#### Gradient implementation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_f(x, f):\n",
    "    #f the function\n",
    "    assert (x.shape[0] >= x.shape[1]), \"the vector should be a column vector\"\n",
    "    x = x.astype(np.float64)\n",
    "    N = x.shape[0]\n",
    "    eps = abs(np.linalg.norm(x) *  np.finfo(np.float32).eps )\n",
    "    gradient = []\n",
    "    f0 = f(x)\n",
    "    for i in range(N):\n",
    "        xx0 = 1. * x[i]\n",
    "        x[i] = x[i] + eps\n",
    "        f1 = f(x)\n",
    "        gradient.append(np.asscalar(np.array(f1 - f0))/eps)\n",
    "        x[i] = xx0\n",
    "    return np.array(gradient).reshape(x.shape)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test gradient :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.00000038],\n",
       "       [ 6.00000038]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we consider the function func defined as xT.dot(T) \n",
    "def func(x):\n",
    "    return np.asscalar(x.T.dot(x))\n",
    "\n",
    "#if x0 =(x1, x2), func(x) = x1² + x2², so grad_f(x0, func) = (2 * x1, 2 * x2)\n",
    "x0 = np.array([1., 3.]).reshape(2,1)\n",
    "gradient_f(x0, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian (x, the_func):\n",
    "    N = x.shape[0]\n",
    "    x = x.astype(np.float64)\n",
    "    hessian = np.zeros((N,N)) \n",
    "    gd_0 = gradient_f(x, the_func)\n",
    "    eps = abs(np.linalg.norm(gd_0) * np.finfo(np.float32).eps )\n",
    "    for i in range(N):\n",
    "        xx0 = 1.*x[i]\n",
    "        x[i] = xx0 + eps\n",
    "        gd_1 =  gradient_f(x, the_func)\n",
    "        hessian[:,i] = ((gd_1 - gd_0)/eps).reshape(x.shape[0])\n",
    "        x[i] =xx0\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Hessian :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.99999958e+00,  -1.66692680e-07],\n",
       "       [ -2.86263754e-07,   1.99999912e+00]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if x0 =(x1, x2), func(x) = x1² + x2²,the hessian matrix should be 2 * I_2 where I_2 is the identity matrix \n",
    "#of dimension 2\n",
    "hessian(x0, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton algorithm :\n",
    "Newton algorithm suppose that the sequence $x_{n+1} = x_n + \\Delta x$ converge to the stationary point $x*$\n",
    "so starting from a random point $x_n$ we iterate to $x_{n+1}$ until we reach $x*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(func, x0):\n",
    "    tol = 1e-4\n",
    "    eps = abs(np.linalg.norm(x0) *  1e-7) \n",
    "    x1 = 1. + x0\n",
    "    while(np.linalg.norm(x1 - x0) > tol):\n",
    "        x0 = x1\n",
    "        grad = gradient_f(x0, func)\n",
    "        x1 = x0 - np.linalg.inv(hessian(x0, func)).dot(grad)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Newton algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.69603615e-09],\n",
       "       [ -9.02101269e-10]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if x0 =(x1, x2), func(x) = x1² + x2², the newton algorithm applied should give the solution (0, 0)\n",
    "Newton(func, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison with scipy\n",
    "We took a scipy example : https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 20\n",
      "         Function evaluations: 21\n",
      "         Gradient evaluations: 20\n",
      "         Hessian evaluations: 19\n",
      "scipy solution is [ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize, rosen, rosen_der, rosen_hess\n",
    "x1 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
    "#Comparaison with scipy\n",
    "def rosen(x):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "def rosen_der(x):\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    der = np.zeros_like(x)\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "    return der\n",
    "\n",
    "def rosen_hess(x):\n",
    "    x = np.asarray(x)\n",
    "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
    "    diagonal = np.zeros_like(x)\n",
    "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
    "    diagonal[-1] = 200\n",
    "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
    "    H = H + np.diag(diagonal)\n",
    "    return H\n",
    "res = minimize(rosen, x1, method='trust-ncg',\n",
    "               jac=rosen_der, hess=rosen_hess,\n",
    "               options={'gtol': 1e-8, 'disp': True})\n",
    "print(\"scipy solution is\",res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implemented solution is\n",
      " [[ 0.99998568]\n",
      " [ 0.99997156]\n",
      " [ 0.99994342]\n",
      " [ 0.99988703]\n",
      " [ 0.99977394]]\n"
     ]
    }
   ],
   "source": [
    "res = Newton(rosen, np.array(x1).reshape(5,1))\n",
    "print(\"implemented solution is\\n\",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 : Newton method (with constraints)\n",
    "\n",
    "#### Method 1 :\n",
    "\n",
    "instead of solving the 1st problem we re-write it by considering the constraint and give the lagrangien function $L(x, \\lambda)$\n",
    "using lagrange multipliers https://en.wikipedia.org/wiki/Lagrange_multiplier, and apply the newton method on \n",
    "the lagrangien function and from the solution $(x*, \\lambda^*)$ select $x^*$\n",
    "\n",
    "#### Method 2 : \n",
    "Using the method explained here https://www.cs.cmu.edu/~ggordon/10725-F12/scribes/10725_Lecture12.pdf\n",
    "we solve the system \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "   H_f(x) & J_h(x)^T \\\\\n",
    "   J_h(x) & 0 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "   \\Delta x \\\\\n",
    "   \\lambda   \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "   -\\nabla f(x)\\\\\n",
    "    -h(x)\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where $J_h(x)$ is the jacobian matrix of the constrainsts of f denoted as $h(x)$ \n",
    "\n",
    "https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant\n",
    "\n",
    "so to find our solution :\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "   \\Delta x \\\\\n",
    "   \\lambda   \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "   H_f(x) & J_h(x)^T \\\\\n",
    "   J_h(x) & 0 \n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "   -\\nabla f(x)\\\\\n",
    "    -h(x)\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Where $h(x) $ are the function of constraints, they are satisfied where $h(x) = 0$\n",
    "if we start from a point $x_k$ that satisfy the constraint the system become \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "   \\Delta x \\\\\n",
    "   \\lambda   \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "   H_f(x) & J_h(x)^T \\\\\n",
    "   J_h(x) & 0 \n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "   -\\nabla f(x)\\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Method implem :\n",
    "\n",
    "We take the introduction example given in wikipedia https://fr.wikipedia.org/wiki/Multiplicateur_de_Lagrange where the aim is to find a portion of the geometric figure which has a volume v0 (constraint) with the minimum surface (function to minimize), our variable are (r:radius,h:height)\n",
    "\n",
    "Function to minimize : \n",
    "\n",
    "$s(r,h) = 2\\pi r(r+h)$\n",
    "\n",
    "Constraint function :\n",
    "\n",
    "$\\phi(r,h) = \\pi r^2h - v_0 = 0$\n",
    "\n",
    "Lagrangien :\n",
    "\n",
    "$L(r,h,\\lambda) = s(r,h) + \\lambda\\phi(r,h)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface(x):#s(r,h)\n",
    "    x = np.abs(x)\n",
    "    return 2 * np.pi * x[0]* x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_vol(x, v0 = 100):#phi(r,h)\n",
    "    return np.pi * x[0]**2 * x[1] - v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrangien(func, N, phis = []): #L(r,h)\n",
    "    return lambda x: np.asscalar(func(x[:N]) + x[N:].T.dot(np.array([phi(x[:N]) for phi in phis])).reshape(len(x[N:]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize r,h,lambda\n",
    "c = np.ones(3).reshape(3,1)\n",
    "#Defining the lagrangien\n",
    "lag = lagrangien(surface, 2, [phi_vol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newton resolution : \n",
    "sol = Newton(lag, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and comments : \n",
    "-According to Wikipedia the solution should be $h = 2r$ here we have a solution where the first component \n",
    "$r = 2.515$ and $h = 5.029$ which seems logic more than that, the evaluation of the constraint = 0 is satisfied\n",
    "\n",
    "-The algorithm is not too stable when tolerance is too small (1e-6) the algorithm do not converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 * r =  [ 5.03174604]\n",
      "h =  [ 5.02900248]\n",
      "the evaluation of the contraint thath should be equal to 0 [ 0.00210852]\n"
     ]
    }
   ],
   "source": [
    "print (\"2 * r = \" ,2 *  sol[0])\n",
    "print (\"h = \" , sol[1])\n",
    "print(\"the evaluation of the contraint thath should be equal to 0\", phi_vol(sol[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d Method implem :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Jacobian matrix\n",
    "def jacobian(x, phis):\n",
    "    jac = []\n",
    "    for phi in phis:\n",
    "        jac.append(gradient_f(x, phi).flatten())\n",
    "    return np.array(jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Matrix Stack\n",
    "def stack_mat(x, func, phis):\n",
    "    J = jacobian(x, phis)\n",
    "    H = hessian(x, func)\n",
    "    HJ = np.hstack((H,J.T))\n",
    "    n = J.shape[0]\n",
    "    Z = np.zeros((n,n))\n",
    "    JZ = np.hstack((J,Z))\n",
    "    stack_mat = np.vstack((HJ, JZ))\n",
    "    return stack_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we consider that the point satisfy the constraints\n",
    "def delta_x_lambda(x, func, phis):\n",
    "    n = len(phis)\n",
    "    return np.linalg.inv(stack_mat(x, func, phis)).dot(-np.vstack((gradient_f(x, func), np.zeros((n,1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the general formula\n",
    "def delta_x_lambda2(x, func, phis):\n",
    "    n = len(phis)\n",
    "    invH = np.linalg.inv(stack_mat(x, func, phis))\n",
    "    minus_gh = np.vstack((-gradient_f(x, func), - np.array([phi(x) for phi in phis]).reshape(n,1)))\n",
    "    return  invH.dot(minus_gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton2(func, x0, phis):\n",
    "    tol = 1e-7\n",
    "    n = x0.shape[0]\n",
    "    x1 = 1. + x0\n",
    "    while(np.linalg.norm(x1 - x0) > tol):\n",
    "        grad = gradient_f(x0, func)\n",
    "        #The step here is calculated diffrently\n",
    "       # print(\"hh\")\n",
    "        dxn = delta_x_lambda2(x0,func, phis)[:n]\n",
    "       # print(dxn)\n",
    "        x1 = x0 + dxn\n",
    "        x0 = x1\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison with scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_eq(x):\n",
    "    return -x[0] + 6 * x[1] - 10\n",
    "def f(x):\n",
    "    # x1² + x2\n",
    "    return x[0] ** 2 + x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraint val: [  3.55271368e-15]\n",
      "solution is  1.6597222222222225\n",
      "constraint val: 1.7763568394e-15\n"
     ]
    }
   ],
   "source": [
    "#Scipy implementation\n",
    "A = np.array([-1, 6])\n",
    "b = 10\n",
    "A_inv = np.linalg.pinv(A.reshape((A.shape[0], 1)))\n",
    "#Feaseable point for scipy\n",
    "x = (b * A_inv).reshape(2,1)\n",
    "print(\"constraint val:\" ,cons_eq(x))\n",
    "cons = ({'type': 'eq', 'fun': cons_eq})\n",
    "sol2 = minimize(f, x, constraints=cons)\n",
    "print(\"solution is \",sol2.fun)\n",
    "print(\"constraint val:\", cons_eq(sol2.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraint val: 4.0\n",
      "solution is  [[-0.0839987 ]\n",
      " [ 1.65266688]]\n",
      "constraint val: [  2.07815809e-09]\n"
     ]
    }
   ],
   "source": [
    "#My implementation with non feaseble point\n",
    "x0 = np.array([1., 2.5])\n",
    "print(\"constraint val:\", cons_eq(x0))\n",
    "sol3 = Newton2(f, x0.reshape(2,1), [cons_eq])\n",
    "print(\"solution is \",sol3)\n",
    "print(\"constraint val:\",cons_eq(sol3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result and comment\n",
    "Here we have solved the linear system with newton descent without feaseble point, but this method didn't work for the first example, we d'ont have the result $h = 2 * r$ confirmed(i post the example below), i d'ont know if it an implementation problem or not, the constraint $h(x) = 0$ is verified when we use the general formula by setting the argument sat of the Newton2 function to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 * r =  [-1.42977294]\n",
      "h =  [ 1.54751892]\n"
     ]
    }
   ],
   "source": [
    "sol2 = Newton2(surface, x0.reshape(2,1), [phi])\n",
    "print (\"2 * r = \" ,2 *  sol2[0])\n",
    "print (\"h = \" , sol2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
